---
title: "Lab 7b - Text Analysis"
author: "Sebastian Montesinos"
date: "For class Thursday, March 24"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
---

```{r setup, include = FALSE}
# load packages
library(tidyverse)
library(kableExtra)

library(janitor)
library(tidytext)
library(wordcloud)
library(textdata)

# set code chunk defaults
knitr::opts_chunk$set(tidy = F, # display code as typed
                      size = "small", # slightly smaller code font
                      message = FALSE,
                      warning = FALSE,
                      comment = "\t") 

# set black & white default plot theme
theme_set(theme_classic()) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')
```

# Lab Purpose  

We want to learn about text analysis, and will continue to focus on Emily Dickinson's poetry which we saw in our Scraping lab. The College has a connection to Emily Dickinson that you might know about. The [Emily Dickinson Museum](https://www.emilydickinsonmuseum.org/) is located within walking distance of the Amherst College campus, and is overseen by Amherst. Past Data Science classes have taken a tour of the museum, but the museum is currently closed for a major restoration project. They do have online programming options available if you'd like to check them out on their website. 

We're going to analyze Dickinson's poetry.  Recall that we scraped the text for her poems from Wikipedia (the final web scraping code used is available in the Lab 5b folder of the class repo in the `scrape-poems.R` script file, which you engaged with in the prep).

We'll learn about functions in the following packages during our lab: 

1. the **tidytext** package, which makes text analysis easier and is consistent with the tools we've been using in the **tidyverse** package;  
2. the **wordcloud** package which allows us to visually represent the text data in word clouds; and  
3. the **textdata** package which allows us to access lexicons for sentiment analysis.  

Make sure you load each package by running the `setup` code chunk above (and that you've installed them if you are working on your own machine!). 

# The Data

We'll be working with the set of Emily Dickinson's poems we scraped from Wikipedia, available in the file "dickinson-poems.txt" (although it's a txt file, we can load this file using `read_csv()`). We'll remove the cases where the poem text is missing.

```{r get-data}
poems <- read_csv("dickinson-poems.txt") %>% 
  filter(text != "Missing") 
```

# Filepaths and Working Directories

We've chatted about this a few times in class and in various emails, but a quick reminder that it's important to know where your data is in relation to your .Rmd and what your working directory is set to in R. This will be very important for your Shiny apps in particular - the apps have to be able to load the data without you setting the working directory every time. 

When you knit an Rmd file, it searches for files *relative to where the Rmd file is located*. Here are four use cases for reading in a file called "my-data.csv" that you can generalize to other situations:

1. The dataset is in the same folder as the Rmd file:

```{r, eval = FALSE}
mydata <- read_csv("my-data.csv")
```

2. The dataset is in a subfolder called "data" within the folder the Rmd file is in:

```{r, eval = FALSE}
mydata <- read_csv("data/my-data.csv")
```

3. The dataset is up a folder relative to the folder the Rmd file is in (e.g., the data are in a folder called "my-project" and the Rmd file is in "my-project/code"):

```{r, eval = FALSE}
mydata <- read_csv("../my-data.csv")
```

4. The dataset is in one subfolder and the Rmd file is in another subfolder of the same directory (e.g., the data has path "my-project/data" and the Rmd file has path "my-project/code"):

```{r, eval = FALSE}
mydata <- read_csv("../data/my-data.csv")
```

When you are coding interactively, you'll want to make sure your working directory is the same as the folder your Rmd file is in so that the filepaths you specify work. You can check your working directory in the console by typing `getwd()` and hitting enter. If it doesn't match, one way to set your working directory in RStudio is to use the **Session** tab, then **Set Working Directory** and use **To Source File Location**. That's the way I've been sharing with many of you. A second way is to navigate to the folder in the **Files** pane, click on the **More** gear menu in that pane, and choose **Set As Working Directory**. 


\newpage

# 1 - Tidying text

In this part of the lab, we'll work through pre-processing a text using the **tidytext** package.

Tokenizing a text is the process of splitting the text from it's full form and splitting it into smaller units (e.g., sentences, lines, words, etc.). We do this with the `unnest_tokens()` function from the **tidytext** package, which takes on two main arguments: `output` and `input`. `output` creates a new variable that will hold the smaller units of text, and `input` identifies the variable in your dataframe that holds the full text. In the process, we get a long version of the dataset.

> part a - Run the code below and view the `poems_words_all` dataset and compare it to the `poems` dataset to see these changes.

```{r tokenize-words}
poems_words_all <- poems %>%
  unnest_tokens(output = word, input = text)
```

The default unit for tokens is a word, but you can specify the `token =` option to tokenize the text by other functions, such as "characters", "ngrams" ($n$ words that occur together), "sentences", or "lines", among other options.

> part b - Try one or more of these alternative options, using the help as a guide, and see how it changes the output (another option has been shown). 

Solution:

```{r alt-tokens}
poems_ngrams <- poems %>%
  unnest_tokens(output = bigram, input = text,
                token = "ngrams", n = 2)
```

## Stop Words

Many commonly used words like "the", "if", and "or" don't provide any insight into the text and are not useful for analysis.  These are called *stop words* and are typically removed from a body of text before analysis. The **tidytext** package  provides a dataframe with stop words from three different lexicons (SMART, snowball, and onix).  We can use this `stop_words` dataset and `anti_join()` to remove all the stop words from our `poems_words_all` dataset.  

```{r rm-stop-words}
data(stop_words)

# First, take a look at the stop_words dataset
head(stop_words)
tail(stop_words)

stop_words %>% 
  count(lexicon)

# Create new dataset since we are removing words
poems_words <- poems_words_all %>%
  anti_join(stop_words, by = "word")

# Explore which stop words were removed
## If you don't want all these words removed, you can modify 
## the stop_words dataframe before `anti_join`ing 
removed <- poems_words_all %>%
  anti_join(poems_words, by = "word") %>%
  count(word) %>%
  arrange(word)
```

\newpage

#  2- Term frequency

Once our text has been pre-processed, we can use functions we already know and love to create a simple descriptive analysis of the term frequency.

> part a - Common words plot. Run the code below to create a simple plot of the 10 most common words used by Emily Dickinson. *Note*: The `slice()` function is used to select a subset of rows from a dataframe.

```{r top-words-plot, fig.width = 3.6, fig.height = 2.8}
poems_words %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  ggplot(aes(x = reorder(word, n), y = n, 
             color = word, fill = word)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", fill = "none") +
  labs(x = NULL,
       y = "Number of instances",
       title = "The most common words in\nEmily Dickinson's poems")
```


> part b - Run the same code but using the `poems_words_all` dataset. What do you notice about this graphic, and what does this tell us about the utility of removing stop words before analysis?

Solution: The graphic using poem_words_all ends up including a lot of small words like 'the' and 'a' that don't tell us anything interesting about the poems. Therefore, it is very useful to remove stop words if you want to run a more substantive analysis.

```{r}
poems_words_all %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  ggplot(aes(x = reorder(word, n), y = n, 
             color = word, fill = word)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", fill = "none") +
  labs(x = NULL,
       y = "Number of instances",
       title = "The most common words in\nEmily Dickinson's poems")
```

# Recap

To recap, it really only took 4 lines of code to get from our scraped dataset to a dataset formatted for plotting word frequencies:

```{r word-freqs}
word_frequencies <- poems %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by = "word") %>%
  count(word, sort = TRUE) 
```

<!-- Did you commit yet? -->

\newpage

# 3 - Word clouds

Word clouds can be used as a quick visualization of the prevalence of words in a corpus.

> part a - Basic word cloud. We can get a bare-bones word cloud using the `wordcloud()` function from the **wordcloud** package. Run the code chunk below to try this out. 

Note: if you get the error: "Error in plot.new() : figure margins too large" or a message "[word] could not be fit on page. It will not be plotted.", try re-adjusting the size of the plotting pane.

Syntax: We are working with packages that do not all recognize the piping operator and do not use some of our other coding conventions. Whenever you use a new package, you need to learn its syntax. Here, we see a common construction - dataset$variable - that is needed to access variables in data sets if you are working in base R. Sometimes there are alternative syntaxes that help bridge the gap (using `with` to specify the data set that the variables are in is demo-ed below).

```{r basic-word-cloud}
# Word cloud will rearrange each time unless seed is set
# Seed is set before each call to wordcloud, generates 3 identical clouds

# Preferred syntax, closest to class coding style we can get
# Using pipes with *with*
set.seed(231)
word_frequencies %>%
  with(wordcloud(words = word, freq = n, max.words = 50))

# *With* syntax without pipes
set.seed(231)
with(word_frequencies, wordcloud(words = word, freq = n, max.words = 50))

# Using base R to reference variables directly
set.seed(231)
wordcloud(words = word_frequencies$word,
          freq = word_frequencies$n, 
          max.words = 50)

```

> part b - Custom word cloud. We can customize the word cloud by mapping the size and color of words to their frequency. Try this out below. 

```{r custom-word-cloud}
# choose color palette from color brewer
mypal <- brewer.pal(10, "Paired")

set.seed(231)
word_frequencies %>%
  with(wordcloud(words = word, 
                 freq = n,
                 min.freq = 20,
                 max.words = 50,
                 # plot the words in a random order
                 random.order = TRUE,
                 # specify the range of the size of the words
                 scale = c(2, 0.3),
                 # specify proportion of words with 90 degree rotation
                 rot.per = 0.15,
                 # colors words from least to most frequent
                 colors = mypal,
                 # font family
                 family = "sans"))
```

> part c - Create your own custom word cloud with *100* words.

Solution:

```{r}
set.seed(231)
word_frequencies %>%
  with(wordcloud(words = word, 
                 freq = n,
                 min.freq = 1,
                 max.words = 100,
                 # plot the words in a random order
                 random.order = TRUE,
                 # specify the range of the size of the words
                 scale = c(2, 0.3),
                 # specify proportion of words with 90 degree rotation
                 rot.per = 0.15,
                 # colors words from least to most frequent
                 colors = mypal,
                 # font family
                 family = "sans"))
```

<!-- Remember to commit and push as you go! -->

\newpage

# 4 - Term frequency-inverse document frequency (tf-idf)

The idea of *tf-idf* is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a corpus. That may seem slightly counter-intuitive - but you still have ways of accessing the commonly used words if that's what you want to check out instead. 

Computing term frequency statistics** The `bind_tf_idf()` function will compute the term frequency (tf), inverse document frequency (idf), and the tf-idf statistics for us. It requires a dataset with one row per word per poem, meaning we need a variable to indicate which poem the word comes from (`title`, in our tokenized dataset), a variable to indicate the word (`word` in the tokenized dataset), and a third variable to indicate the number of times that word appears in that specific poem. We have to compute that count and add it to our dataset to proceed.

> part a - Run the code below to generate the tf-idf statistics for our data. Note that we do not need to remove stop words. Why not?

Solution: You don't need to remove the stop-words because the tf-idf statistics are taking inverses, meaning the stop words are going to be registered as unimportant.

```{r tf-stats}
# Get remaining missing variable
word_freqs_by_poem <- poems %>%
  unnest_tokens(output = word, input = text) %>%
  group_by(title) %>%
  # count does what summarize(n()) does, leaves new variable "n"
  count(word) 

# Compute tf_idf statistics
poems_tfidf <- word_freqs_by_poem %>%
  bind_tf_idf(term = word, 
              document = title, 
              n = n)
```

> part b - Glimpse the tfidf data set and be sure you understand what each column represents.

Solution:

```{r}
glimpse(poems_tfidf)
```

> part c - Visualizing tf-idf. We can visualize the words with the highest 10 tf-idf values for a subset of the poems using the code below. *Change the seed* to get your own 4 randomly selected poems.

Solution:

```{r tf-idf-plot}
set.seed(220) #change this!
poems_subset <- sample(poems$title, size = 4)

top_tfidf <- poems_tfidf %>%
  filter(title %in% poems_subset) %>%
  arrange(desc(tf_idf)) %>%
  group_by(title) %>%
  slice(1:10) %>%
  ungroup()

ggplot(data = top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf,
                             fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ title, ncol = 2, scales = "free") +
  coord_flip() +
  labs(x = NULL, y = "tf-idf")
```

<!-- You probably know what this reminder is for. -->

\newpage

#  5 - Sentiment analysis

What is sentiment analysis?

From [Text Mining with R](https://www.tidytextmining.com/sentiment.html) (Silge & Robinson 2019):

"When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically... One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn't the only way to approach sentiment analysis, but it is an often-used approach."

There are different lexicons that can be used to classify the sentiment of text.  Today, we'll compare two different lexicons that are both based on unigrams, the AFINN lexicon and the NRC lexicon.

The AFINN lexicon (Nielsen 2011) assigns words a score from -5 (negative sentiment) to +5 (positive sentiment).  

> part a - Check out the AFINN lexicon using the code below. What do you think of the scores?  What is the rating for the word "slick"?  Does "slick" always have a positive connotation (can you think of a sentence where "slick" has a negative connotation)?

Solution: 'Slick' could imply that someone is devious or mendacious, so it might sometimes have a negative connotation.

```{r afinn-lex}
# Type "Yes" to download if prompted
afinn_lexicon <- get_sentiments("afinn")

```

> part b - Use the `get_sentiments()` function to create a dataframe `nrc_lexicon` that holds the NRC Word-Emotion Association lexicon (Mohammad 2010). What does each row in this dataset represent? 

Hint: it's *not* the same as the `afinn_lexicon` dataset.

Solution: Each row in this dataset represents a word and then a qualitative measure of that words sentiment ie. 'trust' 'fear' or 'negative'.

```{r}
nrc_lexicon <- get_sentiments("nrc")
```


The NRC lexicon categorizes words as yes/no for the following sentiment categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

> part c - User (and Consumer!) Beware: Do you see any issues in applying these lexicons (developed fairly recently) to the Emily Dickinson poems?

Solution: Emily Dickinson was writing a very long time ago and the connotation of words changes over time: words that were positive could become negative or vice-versa.

 

> part d - The lexicons are based on unigrams.  Do you see any disadvantages of basing the sentiment on single words?

Solution: Sentences often change the meaning of the words therein, and so looking only at specific words might mean one misses important context from the sentences those words are embedded in.



> part e - We can calculate how many words used in the poems are not found in the lexicons using the code below.  List a few words that are not in the NRC lexicon that appear in the poems.  What proportion of unigrams observed within this corpora of Dickinson poems are *not* scored by the NRC lexicon?

Solution: A few examples of words not in the NRC lexicon but in the poems are 'day', 'thee', and 'tis'.

```{r missed-words, eval = FALSE}
# remove this eval = FALSE when compiling
# assumes you called the nrc lexicon nrc_lexicon, as asked above

nrc_missed_words <- word_frequencies %>%
  anti_join(nrc_lexicon, by = "word")

```

> part f - With these (rather important!) drawbacks in mind, let's go ahead and view the top words by sentiment classified by the NRC lexicon. That is, create a figure of the top 10 words under each sentiment, facetted by sentiment, for the following sentiments: anger, anticipation, fear, joy, surprise, and trust.  Use code given in earlier chunks to guide you.

Solution:

```{r}
word_frequencies2 <- word_frequencies %>%
  left_join(nrc_lexicon) %>%
  drop_na(sentiment) %>%
  filter(sentiment == "anger")


word_frequencies2 %>%
  count(n, sort = TRUE) %>% 
  slice(1:10) %>%
  ggplot(aes(x = reorder(word, n), y = n, 
             color = word, fill = word)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", fill = "none") +
  labs(x = NULL,
       y = "Number of instances",
       title = "The most common words in\nEmily Dickinson's poems")
```


> part g - How might you summarize the sentiment of this corpus using the AFINN lexicon?

Solution:

```{r afinn-sentiment}

```

\newpage

# 6 - Regular Expressions

When working with text as data, regular expressions are a powerful tool to find patterns. They have enormous flexibility, but often provide challenges to learners. Do I need a slash? Am I detecting the pattern I'm looking for?

Here, we explore applying some regular expressions to the titles of Emily Dickinson poems. You should think about what would change to apply these to individual poem text. 

First, we get just the titles in their own vector (note, NOT a data frame).

```{r}
titles <- poems$title
```

What poems had the word Bee with a space - "Bee " or "bee " in their title?

```{r}
titles %>% str_subset("(B|b)ee ") %>% head()
```

> part a - Copy and edit the code above to look for "Bee " only. What do you find?

Solution:

```{r}

```

> part b - What if the space was left out of the original expression? What else does this pick up? 

Solution:

```{r}
#Edit as needed
titles %>% str_subset("(B|b)ee ") %>% head()
```

> part c - What does the following regular expression pick up?

Solution:

```{r}
titles %>% str_subset(" [A-Z][a-z][a-z][a-z][a-z][a-z] ") %>% head()
```

> part d - What about the following regular expression?

Solution:

```{r}
titles %>% str_subset("^The") %>% head()
```

> part e - And finally, what about the following regular expression?

Solution:

```{r}
titles %>% str_subset("ing$") %>% head()
```

> part f - Write your own regular expression to look for a pattern of your choosing in the poem titles. Be sure it works as expected.

Solution: 

```{r}

```

<!-- Remember to commit and push your completed lab including the renamed pdf to your personal repo. -->

\newpage

# References 

## AFINN Lexicon 

Nielsen, FA. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May. http://arxiv.org/abs/1103.2903.

## NRC Lexicon 

Mohammad S, Turney P. Crowdsourcing a Word-Emotion Association Lexicon. *Computational Intelligence*. 2013;29(3):436-465.    

Mohammad S, Turney P. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California. http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

## Text Mining with R 

Silge J, Robinson D (2016). "tidytext: Text Mining and Analysis Using Tidy Data Principles in R." *JOSS*, *1*(3). doi: [10.21105/joss.00037](https://doi.org/10.21105/joss.00037).

Silge J, Robinson D (2017). Text Mining with R: A Tidy Approach. O'Reilly Media Inc. Sebastopol, CA. https://www.tidytextmining.com/index.html
