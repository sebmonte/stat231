---
title: "Lab 9 - Spatial Data"
author: "YourNameGoesHere"
date: "For class the week of April 12"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
---

```{r setup, include = FALSE}
# load packages
library(tidyverse)
library(kableExtra)

library(sf)
library(viridis)
library(leaflet)

# set code chunk defaults
knitr::opts_chunk$set(tidy = F, # display code as typed
                      size = "small", # slightly smaller code font
                      message = FALSE,
                      warning = FALSE,
                      comment = "\t") 

# set black & white default plot theme
theme_set(theme_classic()) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')
```

# Lab Purpose  

This lab will enable us to practice working with spatial data. As you saw in the textbook reading and prep, working with spatial data often means making and customizing maps. 

The packages we'll be working with for mapping are:

* maps: provides spatial data files for the world (`world`, `world.cities`, `lakes`), the US (`county`, `state`, `usa`, `us.cities`), France (`france`), Italy (`italy`), and New Zealand (`nz`). There is generally no need to load this package (the shapefiles are already loaded with **ggplot2**); 
* [sf](https://r-spatial.github.io/sf/): provides support for "simple features" objects, the standard data containers for spatial data; 
* leaflet: allows us to create dynamic maps. If you are thinking about interactive maps for the final project, this is what you could use in Shiny. 

Here are some additional potentially useful packages for working with spatial data or for obtaining shapefiles:

* [ggspatial](https://paleolimbot.github.io/ggspatial/reference/index.html): adds additional annotations, geometries, and layers for building onto static maps made with `ggplot()`; and
* mapproj: converts latitude and longitude into projected coordinates, primarily with `mapproject()`.
* mapdata: an add-on to the **maps** package (includes `china`, `japan`, and `world2Hires` for a Pacific-centric world map); 
* rnaturalearth: provides easy access to public domain map datasets from the Natural Earth project (tend to be higher resolution than data from the **maps** package)
* oz: map data for Australia and Australian states; and  
* [urbnmapr](https://github.com/UrbanInstitute/urbnmapr#readme): US Census Bureau shapefiles (counties, states, territories) 

**IMPORTANT**

Working with maps can generate many objects and the maps can take up a good bit of space/memory. In order to be able to view the objects easily, we want to keep our workspace clean. Before running any of the map code below, head to your Environment window, and hit the "broom" button. This will clear out your workspace. You will need to reload the package chunk at the top. In the lab below, due to the number of objects being generated, you will see another way to remove specific items from the Environment. 

Cleaning out your workspace regularly is good practice. If it's always empty when you start working, it reduces the chance you'll run into issues with code "working, but at the same time not working" on saved objects that aren't loaded in the current .Rmd. Example: the data set from wrangling is in your Environment, but you're in a new .Rmd and didn't load it in. The new .Rmd won't compile, and you don't understand why. If the Environment was empty to start, it would remind you that you'd need to load in the data set. 

\newpage

# 1 - Using spatial data from the maps package

The **maps** package is loaded with **ggplot2** when we load **tidyverse** and provides a very limited set of map data . There are two ways to work with data from the maps package, outlined below.  

The first approach require use of the `map_data()` function from **ggplot2**, which turns the spatial data from the **maps** package into a data frame. The first argument, `map` takes the shapefile of interest, and the second argument, `region`, can be used to identify subregions to include (the default is  `region = "."`, which includes all subregions). To use this data, we add a `geom_polygon()` layer to the ggplot.

> part a - Run the code below to convert the `world` map data into a dataframe, take a peak at what the dataframe looks like, and then plot it with `ggplot()`.

```{r}
# Get a dataframe with longitude and latitude
world_map_df <- map_data(map = "world")

head(world_map_df)
tail(world_map_df)

ggplot(world_map_df, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "lightgrey", color = "white") +
  # Use empty theme to remove background color, axes, ticks
  theme_void()  
```

As an alternative, we turn the **maps** spatial data into an `sf` object and make use of a `geom_sf()` layer with `ggplot()`. This approach correctly preserves the aspect ratio of the map.

> part b - Run the code below to convert the `world` map data into a an `sf` object using `st_as_sf()` (from the **sf** package), take a peak at what this `sf` object looks like, and then plot the map with `ggplot()`.

Note: Because of the more common use of the `map()` function from **purrr**, it is good practice to explicitly identify the **maps** package when using its `map()` function.

```{r}
# Often preferred because it preserves aspect ratio
# Obtain sf object of world map
world_map <- maps::map("world", plot = FALSE, fill = TRUE) %>% 
  st_as_sf()

head(world_map)

ggplot(data = world_map) +
  geom_sf(fill = "lightgrey", color = "white") +
  theme_void()
```

> part c - Your turn! Use data from the **maps** package to create a plot of New Zealand.

Solution:

```{r}

```

> part d - The **maps** package also allows us to plot counties (and states) within the US. Run the code below to get a county-level map for Massachusetts.

```{r}
# Identify appropriate region value from county map data
county_df <- map_data("county") 
# Run in console to see how data appear: View(county_df)

# Create sf object for Massachusetts counties
ma_map <- maps::map("county", regions = "massachusetts",
                    plot = FALSE, fill = TRUE) %>% 
  st_as_sf()

ggplot(data = ma_map) +
  geom_sf(fill = "lightgrey", color = "white") +
  theme_void()
```

> part e - See if you can create a light grey map of the US with states outlined in black and counties outlined in white.

Hints: remember that additional data can be read in within a geometry, and `fill = NA` may come in handy. For example, store county info in one data set and state information in another, then plot both. 

Solution:

```{r}

```


```{r, echo = FALSE}
# cleaning environment of everything but state map information before continuing
rm(county_df, 
   county_map, # edit as needed if you named objects differently
   ma_map,
   nz_map,
   world_map,
   world_map_df)
```

\newpage

# 2 - Choropleths

We may at times want to shade or color regions based on the value of a variable to create a map called a *choropleth*. Usually the process to do this is:

1. Identify a data source that provides the correct spatial information needed (this may be in the form of a shapefile already, as in the **maps** package, but this can be hard to find)
2. Identify a data source that provides the variable of interest for coloring the map
3. Join the two sources together (usually much harder than it sounds!)
4. Create the map!

Let's try this with data collected in July 2020 by [The Chronicle for Higher Education](https://www.chronicle.com/article/heres-a-list-of-colleges-plans-for-reopening-in-the-fall) and Davidson College's College Crisis Initiative (C2i) on colleges' reopening plans for Fall 2020. Our goal is to make a map of the US, with each state colored by the proportion of institutions in the state that were planning to be in person for Fall 2020.

We've already prepared the state spatial data (in e of Part 1), so now we are ready to prepare the Chronicle data. Run the code below to load the Chronicle data, conduct a little data wrangling to extract each school's plan (buried in the "X.1" column), and get a summary dataset with one row per state. 

> part a - Take the time to look at the datasets that are created and try to make sense of the steps that were taken.

```{r}
# Load data
college_plans <- read_csv("data/chronicle_plans.csv")

# Wrangle
college_plans <- college_plans %>% 
  # Some plans are embedded within HTML tags within X.1 variable
  # so we want to extract all the text between ">" and "<"
  extract(col = X.1, into = "plans_extracted",
          regex = ">(.*)<", remove = FALSE) %>% 
  # Combine extracted text with plain text of plans from X.1
  mutate(plans = case_when(is.na(plans_extracted) ~ X.1,
                           TRUE ~ plans_extracted)) %>% 
  # Remove rows without plans
  filter(plans != "Link")

head(college_plans)

# Check summary of plans
college_plans %>% count(plans)

# Count colleges per state 
colleges_per_state <- college_plans %>% 
  count(State) %>%
  rename(n_colleges = n)

head(colleges_per_state)

# Count colleges in-person per state 
college_plans_per_state <- college_plans %>% 
  count(State, plans) %>% 
  # Fill in 0s as needed (e.g., no schools in a state have in-person plans)
  ungroup() %>% 
  complete(State, plans, fill = list(n = 0)) %>% 
  filter(plans == "Planning for in-person") %>%
  rename(n_in_person = n)

head(college_plans_per_state)

# Join for final dataset
college_plan_summary <- colleges_per_state %>% 
  left_join(college_plans_per_state) %>% 
  mutate(proportion_in_person = n_in_person/n_colleges)

head(college_plan_summary)
```

<!-- Decluttering ------------------------------------------------------------->
```{r, echo = FALSE}
# clean up environment
rm(colleges_per_state, college_plans_per_state)
```

Next, we combine the state-level college planning information with the state-level mapping information.  The Chronicle of Higher Education dataset has a variable that contains two-letter abbreviations for states (e.g., "MA") whereas the state variable in the mapping dataset includes the full name of the state in lowercase letters (e.g. "massachusetts").  

We can use the `state` datasets available in base R package (also used in a previous lab) to connect the state abbreviations to the state names. 

> part b - Again, take the time to look at the datasets that are created and try to make sense of the steps that were taken.

<!--
I called my state-level mapping data `state_map`. You should update the code below to match your dataset name from Part 1.e
-->
```{r}
# Peek at data
head(college_plan_summary)
head(state_map)

# State objects should appear in your Environment pane
data(state) 

# Create data frame with state info
state_info <- data.frame(Region = state.region,
                         # Match state variable name in map data
                         ID = tolower(state.name), 
                         # Match state variable name in summary data
                         State = state.abb)
head(state_info)

# Join datasets from the left starting with the sf object ()
college_plans_map <- state_map %>% 
  left_join(state_info) %>% 
  left_join(college_plan_summary)

head(college_plans_map)
```

<!-- Decluttering ------------------------------------------------------------->
```{r, echo = FALSE}
# clean up environment
rm(state_info, college_plan_summary)
```

> part c - Now, we can create a choropleth with `ggplot()`! Customize the plot below with a color palette of interest to you and useful labels and titles.

<!-- 
Remember we have several options for changing the color palette, e.g.:

scale_fill_distiller(palette = "...")  
scale_fill_viridis(option = "...", direction = -1)  (requires the **viridis** package)
scale_fill_brewer(palette = ...) 
-->

Solution:

```{r}
ggplot(college_plans_map, aes(fill = proportion_in_person)) +
  geom_sf() +
  scale_fill_viridis(option = "magma", direction = -1) +
  theme_void()
```
  
Instead of summarizing the college plans across institutions in a given state, we may want to plot a point for every college and add a visual cue to indicate each individual college plan (e.g., color the points by plan category). This requires getting spatial data for each institution, and then adding a layer to our graph. This information is not included in the Chronicle dataset, so we need to find it from somewhere else.  The National Center for Education Statistics collects detailed location information for all of the higher education institutions in the US, and makes the data publicly available through [IPEDS](https://nces.ed.gov/ipeds/datacenter/Data.aspx). 

> part d - Run the code below, again taking the time to look at the datasets that are created and try to make sense of the steps that were taken.

```{r}
colleges <- read_csv("data/ipeds_directory_info.csv") %>%
  janitor::clean_names() %>% 
  select(long = longitude_location_of_institution_hd2019,
         lat = latitude_location_of_institution_hd2019,
         Institution = institution_name,
         Type = control_of_institution_hd2019) %>% 
  mutate(Type = factor(Type, 
                       levels = c(1,2,3),
                       labels = c("Public", 
                                  "Private, Not-for-profit",
                                  "Private, For-profit"))) %>% 
  # Make sure coordinate projection matches our data
  st_as_sf(coords = c("long", "lat"), 
           crs = 4326, agr = "constant")

colleges_map <- colleges %>% 
  right_join(college_plans) %>% 
  # State map is only of contiguous US (sorry, Alaska and Hawaii!!)
  filter(!(State %in% c("AK", "HI")))

ggplot(college_plans_map) +
  geom_sf(aes(fill = proportion_in_person)) +
  scale_fill_viridis(option = "magma", direction = -1) +
  geom_sf(data = colleges_map, aes(color = plans)) + 
  theme_void() +
  labs(fill = "Proportion",
       color = "Plans",
       title = "Proportion of colleges planning for in-person learning for Fall 2020, by state",
       subtitle = "as of July 2020") +
  theme(legend.position = "bottom")
```

> part e - Not all of the institutions in the Chronicle of Higher Education's file matched to an institution in the IPEDS file.  For instance, the Chronicle file has one row for Arizona State University, but the IPEDS file but has multiple rows for the same university to represent the location of the different campuses.  What other types of mismatches are there? Can you think about how to clean up the mismatches?

Solution:

```{r}
# for you to explore mismatches
# don't work to fix them, just strategize
```

\newpage

# 3 - Your Turn - Create a map

Create a map of your choosing to display either country-level data on a world map, state-level data on a country map (keeping in mind the limitations of the **maps** package and of your time to figure out one of the other shapefile packages mentioned), or county-level data on a state map.

Don't spend too long looking for unit-level data you're interested in.  I strongly recommend you use data readily available in an R package below (some sample code is provided to help you get started).  For instance:

* the `gapminder` dataset from the **gapminder** package has (a few) country-level variables
* the `hate_crimes` dataset from the **fivethirtyeight** package has state-level variables
* the `states` data from base R has a matrix of state-level variables in the object `state.x77`
* example county-level data file - on unemployment from [the USDA](https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/) in the *data* subfolder. 

Be sure that your figure has an appropriate title and legend for context, etc.

This map will be used for homework in Practice7. 

<!-- 
If you use any of the code below, copy and paste only the code you need into a new code chunk for wrangling and then mapping your data
-->

```{r sample-code, eval = FALSE}
# Example of dataset with country information across years
library(gapminder)
data(gapminder)
head(gapminder)

# Example of dataset with state information from 2013-2014
library(fivethirtyeight)
data(hate_crimes)
head(hate_crimes)

# Example of another states dataset from Base R with state information from 1977
states_1977 <- data.frame(state.x77) %>%
  rownames_to_column(var = "State") %>%
  janitor::clean_names()

# Example of dataset with county-level information from 2019
# (see second tab in excel file for variable explanations)
county_employment <- readxl::read_xls("data/usda-unemployment.xls",
                                      sheet = 1,
                                      skip = 7) %>%
  janitor::clean_names()
```

Solution:

```{r your-map}

```


\newpage


# 4 - Leaflet

Explore interactivity with **leaflet**. Modify the example with Fall 2020 college reopening plans below (can you figure out how to fill the state color by proportion of colleges in person?), or try your hand at making your map from Part 3 interactive.  

<!-- 
Interactive graphs will not knit to pdf so set `eval = FALSE` in the following code chunk
-->
```{r, eval = FALSE}
# Define a color palette over the values 0 to 1 (for proportion in person )
mypal <- colorNumeric(palette = "YlGnBu", domain = c(0,1))

# Identify Amherst College's location and pull the corresponding coordinates
ac <- colleges %>%
  filter(Institution == "Amherst College") %>% 
  pull(geometry) %>% 
  # Convert geometry coordinates to coordinate matrix
  st_coordinates()

# Create interactive map
leaflet(data = college_plans_map) %>% 
  addTiles() %>%
  addMarkers(lat = ac[1], lng = ac[2], popup = "Amherst College") %>%
  addPolygons(fillColor = topo.colors(10, alpha = NULL), 
              stroke = FALSE,
              popup = ~ paste0("State: ", ID %>% str_to_title(), "<br>",
                               "Number of schools reporting: ", n_colleges, "<br>",
                               "Number of schools planning for in-person learning: ",
                                  n_in_person, "<br>",
                               "Proportion planning for in-person learning: ",
                               proportion_in_person %>% round(2))) %>% 
  setView(lng = ac[1], lat = ac[2], zoom = 6) 
```

<!-- Remember to commit and push your final version of the lab. -->

